# LLM Dashboard â€” Frontend

A full-featured web dashboard for managing self-hosted LLM deployments via **Ollama** (CPU) and **vLLM** (GPU).

Built with **Next.js 14** Â· **shadcn/ui** Â· **Tailwind CSS** Â· **TypeScript**

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ and **npm**
- **Ollama** installed and running (`ollama serve`)
- (Optional) **vLLM** for GPU deployments

### One-Command Setup

From the repo root:

```bash
chmod +x setup.sh && ./setup.sh
```

This installs dependencies and starts the dev server.

### Manual Setup

```bash
cd frontend
npm install
npm run dev
```

Open **http://localhost:3000** in your browser.

---

## ğŸ“– Usage Guide

### Dashboard (`/`)

The home page shows a live overview:

- **Stats Cards** â€” Models deployed, models running, RAM usage, active mode (CPU/GPU)
- **Deployed Models** â€” Quick list of all models with status indicators
- **Recent Activity** â€” Log of deploy, start, stop, and delete events
- **Quick Actions** â€” "Deploy Model" and "Open Chat" buttons

### Deploy (`/deploy`)

Deploy a new model in two modes:

| Mode | Tab | What it does |
|------|-----|-------------|
| **CPU** | CPU (Ollama) | Enter a HuggingFace repo name â†’ runs `deploy_cpu.sh` â†’ auto-selects best quantization |
| **GPU** | GPU (vLLM) | Enter model name + GPU slot (0 or 1) â†’ runs `deploy_model.sh` |

- Real-time deployment progress is streamed via SSE (Server-Sent Events)
- Quantization reference table shows Q2_K through Q8_0 options

### Models (`/models`)

Manage all deployed models in a table:

| Column | Description |
|--------|-------------|
| **Model** | Model name |
| **Type** | CPU or GPU badge |
| **Status** | ğŸŸ¢ Running / ğŸ”´ Stopped |
| **Size** | Model file size |
| **Quantization** | Quant level (e.g., Q4_K_M) |
| **API Endpoint** | Clickable URL with copy button |
| **Actions** | â–¶ Start Â· â¹ Stop Â· ğŸ—‘ Delete |

### Chat (`/chat`)

Interactive chat with any running model:

- **Conversation sidebar** â€” Create, switch, delete conversations
- **Model selector** â€” Pick from running models (shows status badge)
- **Streaming responses** â€” Token-by-token output from Ollama
- **Persistence** â€” Conversations saved in `localStorage`

### System (`/system`)

System hardware and service health:

- **OS / CPU / RAM / Hostname** cards
- **Service Health** â€” Ollama and vLLM status with health badges
- **Memory Usage** chart (Recharts bar chart)

---

## ğŸ§ª Running Tests

```bash
# Unit tests (Vitest)
npm run test

# Watch mode
npm run test -- --watch

# E2E tests (Playwright) â€” requires dev server running
npx playwright test
```

**Current: 47/47 tests passing** across validators, utils, API client, and components.

---

## ğŸ— Production Build

```bash
npm run build
npm start
```

---

## ğŸ“ Project Structure

```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                    # Next.js App Router pages & API routes
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/           # POST â€” streaming chat proxy
â”‚   â”‚   â”‚   â”œâ”€â”€ deploy/         # POST â€” start deploy + GET SSE stream
â”‚   â”‚   â”‚   â”œâ”€â”€ health/         # GET â€” Ollama/vLLM health
â”‚   â”‚   â”‚   â”œâ”€â”€ models/         # GET list, GET/DELETE [name], POST start/stop
â”‚   â”‚   â”‚   â””â”€â”€ system/         # GET â€” OS/CPU/RAM info
â”‚   â”‚   â”œâ”€â”€ chat/page.tsx
â”‚   â”‚   â”œâ”€â”€ deploy/page.tsx
â”‚   â”‚   â”œâ”€â”€ models/page.tsx
â”‚   â”‚   â”œâ”€â”€ system/page.tsx
â”‚   â”‚   â””â”€â”€ page.tsx            # Dashboard
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ dashboard/          # StatsCards, ModelSummary, ActivityLog
â”‚   â”‚   â”œâ”€â”€ layout/             # Sidebar, Header, ModeToggle
â”‚   â”‚   â”œâ”€â”€ system/             # SystemOverview, ResourceChart
â”‚   â”‚   â””â”€â”€ ui/                 # shadcn/ui components
â”‚   â”œâ”€â”€ hooks/                  # useModels, useSystemInfo, useDeploy, useSSE
â”‚   â””â”€â”€ lib/                    # api.ts, types.ts, constants.ts, validators.ts, utils.ts
â””â”€â”€ __tests__/                  # Vitest unit + API tests
```

---

## âš™ï¸ Configuration

Key constants are in `src/lib/constants.ts`:

| Constant | Default | Description |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama API address |
| `VLLM_PORTS` | `{0: 8104, 1: 8105}` | vLLM ports per GPU slot |
| `POLL_INTERVAL.models` | `10000` | Model list refresh (ms) |
| `POLL_INTERVAL.health` | `15000` | Health check refresh (ms) |

---

## ğŸ›¡ Security

- All model names are validated with Zod against command injection (blocks `;`, `|`, `` ` ``, `$()`, etc.)
- API input validation on every route
- No direct shell command construction from user input
